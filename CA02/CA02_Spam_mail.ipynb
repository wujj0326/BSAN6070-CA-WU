{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA02_Spam_mail.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMedFTYDmMXY9ctlfHantan"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbeqfUkFeelV",
        "colab_type": "text"
      },
      "source": [
        "# CA02\n",
        "This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SuCfhZ5Xb7L",
        "colab_type": "text"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxPD4caAOGEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyKX2lz4fUih",
        "colab_type": "text"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1R5sO3texru",
        "colab_type": "text"
      },
      "source": [
        "This function builds a Dictionary of most common 3000 words from all the email content. First it adds all words and symbols in the dictionary. Then it removes all non-alpha-numeric characters and any single character alpha-numeric characters. After this is complete it shrinks the Dictionary by keeping only most common 3000 words in the dictionary. It returns the Dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5w8lo6QezQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_Dictionary(root_dir):\n",
        "  all_words = []\n",
        "  # Join all the file(f) in the directory \"root_dir\" to the path \"root_dir\"\n",
        "  # \"emails\" is now a list of path of the txt files\n",
        "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]\n",
        "  for email in emails:\n",
        "    # open each txt file\n",
        "    with open(email) as m:\n",
        "      for line in m:\n",
        "        # seperate each line to words\n",
        "        words = line.split()\n",
        "        # Use += instead of directly appending to \"all_words\" because a \"words\" is a list of words in a line,\n",
        "        # therefore, it would append multiple lists to \"all_words\"\n",
        "        all_words += words\n",
        "  # Counter is a function that counts each element in a list\n",
        "  valid_word = Counter(all_words)\n",
        "  # A list of the unique words in the Counter\n",
        "  list_to_remove = list(valid_word)\n",
        "\n",
        "  for item in list_to_remove:\n",
        "    # isalpha() is a method that check if the argument contains any alphabets characters\n",
        "    if item.isalpha() == False:\n",
        "      # delete the words that are not alphabets characters\n",
        "      del valid_word[item]\n",
        "    elif len(item) == 1:\n",
        "      # delete the words that equal to 1 character\n",
        "      del valid_word[item]\n",
        "  # a list of the n most common elements and their counts from the most common to the least.\n",
        "  valid_word = valid_word.most_common(3000)\n",
        "  return valid_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQkyTVYCfCNq",
        "colab_type": "text"
      },
      "source": [
        "This function extracts feature columns and populates their values (Feature Matrix of 3000 columms and rows equal to the number of email files). The function also analyzes the File Names of each email file and decides if it's a Spam or not based on the naming convention. Based on this the function also creates the Labelled Data Column. This function is used to extract the training dataset as well as the testing dataset and returns the Feature Dataset and the Label column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkjVdL1EfDKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_features(mail_dir):\n",
        "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
        "  features_matrix = np.zeros((len(files),3000))\n",
        "  train_labels = np.zeros(len(files))\n",
        "  docID = 0;\n",
        "  for fil in files:\n",
        "    with open(fil) as fi:\n",
        "      # Give each line of text an index -> 'i' is the index starting from 0 and 'line' is the element\n",
        "      # ex. [(0,\"subject\"),(1,\"blank\"),(2,\"content\")]\n",
        "      for i, line in enumerate(fi):\n",
        "        # the content is in the third element which the index==2\n",
        "        if i ==2:\n",
        "          # split the line to words\n",
        "          words = line.split()\n",
        "          for word in words:\n",
        "            # set the wordID as an integer 0\n",
        "            wordID = 0\n",
        "            # Give each word in the valid_word an index -> 'i' is the index starting from 0 and 'd' is the word\n",
        "            # ex. [(0,\"word1\"),(1,\"word2\").....(2999,\"word2999\")]\n",
        "            for i, d in enumerate(valid_word):\n",
        "              # d[0] is the word in valid_word and d[1] is the count of the word\n",
        "              # so if \"word\" have shown up in valid_word which is d[0] then:\n",
        "              if d[0] == word:\n",
        "                # the wordID would be the index of the word in valid_word\n",
        "                wordID = i\n",
        "                # let's assume we are dealing with the first document and the \"word\" in \"words\" have all shown up in a enumerate(valid_word)\n",
        "                # that looks like [(0,'to'),(1,'be'),(2,'or'),(3,'not')]\n",
        "                # -> the parameters would be: docID = 0, wordID=0, words=['to','be','or','not','to','be']\n",
        "                # -> feature_matrix[0,0]=words.count('to'), feature_matrix[0,1]=words.count('be'),\n",
        "                #    feature_matrix[0,3]=words.count('or'), feature_matrix[0,4]=words.count('not')\n",
        "                # ->            0(to) 1(be) 2(or) 3(not) \n",
        "                #   0(1st doc) [2,    2,    1,    1]\n",
        "                # * It's the frequency of each word in valid_word in nth document.*\n",
        "                features_matrix[docID,wordID] = words.count(word)\n",
        "      # train_labels[docID] = 0; # train_labels is already set up as a np.zero array\n",
        "      # split the \"fil\" by \"/\"\n",
        "      filepathTokens = fil.split('/')\n",
        "      # Python's index start from 0, so the len()-1 would be the index of last token\n",
        "      lastToken = filepathTokens[len(filepathTokens)-1]\n",
        "      if lastToken.startswith(\"spmsg\"):\n",
        "        train_labels[docID] = 1;\n",
        "      # jump to the next document\n",
        "      docID = docID + 1\n",
        "  return features_matrix, train_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4tLzejifa8H",
        "colab_type": "text"
      },
      "source": [
        "## Main Program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG40pqOZfIyh",
        "colab_type": "text"
      },
      "source": [
        "The section is the main Program that calls the above two functions and gets executed first. First it \"trains\" the model using model.fit function and Training Dataset. After that it scores the Test Data set by running the Trained Model with the Test Data set. At the end it prints the model performance in terms of accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIA5Opp3f15K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrulU3PMfOD3",
        "colab_type": "code",
        "outputId": "940503e0-672f-49ea-9d80-3a95c1853150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "TRAIN_DIR = '/content/drive/My Drive/MSBA_Colab_2020/ML_Algorithms/CA02/Data/train-mails'\n",
        "TEST_DIR = '/content/drive/My Drive/MSBA_Colab_2020/ML_Algorithms/CA02/Data/test-mails'\n",
        "\n",
        "print (\"Find out all the valid and unique words in the TRAIN folder\")\n",
        "valid_word = make_Dictionary(TRAIN_DIR)\n",
        "\n",
        "print (\"Reading and processing emails from TRAIN and TEST folders\")\n",
        "features_matrix, labels = extract_features(TRAIN_DIR)\n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR)\n",
        "\n",
        "model = GaussianNB()\n",
        "\n",
        "print (\"Training Model using Gaussian Naibe Bayes algorithm .....\")\n",
        "# X: features_matrix, y: labels\n",
        "model.fit(features_matrix, labels)\n",
        "print (\"Training completed\")\n",
        "print (\"Testing trained model to predict Test Data labels\")\n",
        "# put the X_test to the model for prediction\n",
        "predicted_labels = model.predict(test_features_matrix)\n",
        "print (\"Completed classification of the Test Data .... \")\n",
        "print (\"Now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "# inspect the accuracy by checking the real X_label with predicted_X_label\n",
        "print (accuracy_score(test_labels, predicted_labels))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Find out all the valid and unique words in the TRAIN folder\n",
            "Reading and processing emails from TRAIN and TEST folders\n",
            "Training Model using Gaussian Naibe Bayes algorithm .....\n",
            "Training completed\n",
            "Testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... \n",
            "Now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9615384615384616\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}